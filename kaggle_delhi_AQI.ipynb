{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_cities(data):\n",
    "    cleaned_data = data.dropna()\n",
    "    # make a separate dataframe for each city\n",
    "    amaravati = cleaned_data[cleaned_data.City == 'Amaravati']\n",
    "    amritsar = cleaned_data[cleaned_data.City == 'Amritsar']\n",
    "    chandigarh = cleaned_data[cleaned_data.City == 'Chandigarh']\n",
    "    delhi = cleaned_data[cleaned_data.City == 'Delhi']\n",
    "    gurugram = cleaned_data[cleaned_data.City == 'Gurugram']\n",
    "    hyderabad = cleaned_data[cleaned_data.City == 'Hyderabad']\n",
    "    kolkata = cleaned_data[cleaned_data.City == 'Kolkata']\n",
    "    patna = cleaned_data[cleaned_data.City == 'Patna']\n",
    "    visakhapatnam = cleaned_data[cleaned_data.City == 'Visakhapatnam']\n",
    "    return amaravati, amritsar, chandigarh, delhi, gurugram, hyderabad, kolkata, patna, visakhapatnam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, RobustScaler, StandardScaler\n",
    "\n",
    "def preprocess_data(X, y, scale_method = 'default'):\n",
    "    if scale_method == 'default':\n",
    "        y = y/y.max()\n",
    "        X = X/X.max()\n",
    "    else:\n",
    "        if scale_method == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scale_method == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scale_method == 'maxabs':\n",
    "            scaler = MaxAbsScaler()\n",
    "        elif scale_method == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid scaling method\")\n",
    "        X = scaler.fit_transform(X)\n",
    "        y = y.values\n",
    "        y = y.reshape(-1,1)\n",
    "        y = scaler.fit_transform(y)\n",
    "        \n",
    "    if scale_method == 'default':\n",
    "        return X, y, None\n",
    "    else:\n",
    "        return X, y, scaler\n",
    "    \n",
    "def clean_data(data):\n",
    "    cleaned_data = data.dropna()\n",
    "    gasses = cleaned_data.select_dtypes(include = np.float64)\n",
    "    corr = gasses.corr().AQI\n",
    "    col_to_drop = corr[abs(corr) < 0.45].index\n",
    "    gasses = gasses.drop(columns = col_to_drop)\n",
    "    y = gasses.AQI\n",
    "    X = gasses.drop(columns = 'AQI')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def print_model_performance(mse, r2, mae, mape, model_name = 'Linear Regression', scale_method = 'default'):\n",
    "    print(\"\\n For Model: {} with Scale Method: {}\".format(model_name, scale_method))\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    print(\"R-squared:\", r2)\n",
    "    print(\"Mean Absolute Error:\", mae)\n",
    "    print(\"Mean Absolute Percentage Error:\", mape)\n",
    "    \n",
    "def log_results(model_name, scale_method, split, mse, r2, mae, mape, city=\"\"):\n",
    "    # initialize an empty df to concat into later\n",
    "    base_df = pd.DataFrame(columns = ['Model', 'Scale Method', 'Data Split', 'City', 'MSE', 'R2', 'MAE', 'MAPE'])\n",
    "    # in the df we store the model name, the scaling method, datasplit, and the mae, mse, r2, and mape\n",
    "    new_df = pd.DataFrame([[model_name, scale_method, split, city, mse, r2, mae, mape]], columns = ['Model', 'Scale Method', 'Data Split', 'City', 'MSE', 'R2', 'MAE', 'MAPE'])\n",
    "    # if the file exists, we read it in and concat the new results\n",
    "    if os.path.exists('patchresults.csv'):\n",
    "        base_df = pd.read_csv('patchresults.csv')\n",
    "        base_df = pd.concat([base_df, new_df])\n",
    "        base_df.to_csv('patchresults.csv', index = False)\n",
    "    # if the file doesn't exist, we create a new one\n",
    "    else:\n",
    "        new_df.to_csv('patchresults.csv', index = False)\n",
    "\n",
    "def plot_predictions(y_test, y_pred, scale_method = 'default', model_name = 'Linear Regression', split=0.3, city = \"\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    y_test = np.array(y_test)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_test = y_test[:int(split * len(y_test))]\n",
    "    y_pred = y_pred[:int(split * len(y_pred))]\n",
    "    plt.plot(y_test, label='True')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.title(\"True vs Predicted AQI of {} using {} with Scale Method: {}\".format(city, model_name, scale_method))\n",
    "    plt.xlabel(\"Observation\")\n",
    "    plt.ylabel(\"AQI\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_model(data, scale_method = 'default', plot_split=0.3, data_split=0.2, city=\"\"):\n",
    "    X, y = clean_data(data)\n",
    "    # y_max = y.max()\n",
    "    X, y, scaler = preprocess_data(X, y, scale_method)\n",
    "    X_train = X[:int(data_split * len(X))]\n",
    "    X_test = X[int(data_split * len(X)):]\n",
    "    y_train = y[:int(data_split * len(y))]\n",
    "    y_test = y[int(data_split * len(y)):]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    print_model_performance(mse, r2, mae, mape, model_name='Linear Regression', scale_method=scale_method)\n",
    "    plot_predictions(y_test, y_pred, scale_method=scale_method, model_name='Linear Regression', split=plot_split, city=city)\n",
    "    log_results('Linear Regression', scale_method, data_split, mse, r2, mae, mape, city=city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def nn_model(data, scale_method = 'default', plot_split = 0.3, data_split=0.2, city=\"\"):\n",
    "    X, y = clean_data(data)\n",
    "    # y_max = y.max()\n",
    "    X, y, scaler = preprocess_data(X, y, scale_method)\n",
    "    X_train = X[:int(data_split * len(X))]\n",
    "    X_test = X[int(data_split * len(X)):]\n",
    "    y_train = y[:int(data_split * len(y))]\n",
    "    y_test = y[int(data_split * len(y)):]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[es], verbose=0)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)   \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    try:\n",
    "        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    except:\n",
    "        # convert to one dimensional array\n",
    "        y_test = np.array(y_test)\n",
    "        y_pred = np.array(y_pred)\n",
    "        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    print_model_performance(mse, r2, mae, mape, model_name='Neural Network', scale_method=scale_method)\n",
    "    plot_predictions(y_test, y_pred, scale_method=scale_method, model_name='Neural Network', split=plot_split, city=city)\n",
    "    log_results('Neural Network', scale_method, data_split, mse, r2, mae, mape, city=city)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def lstm_model(data, scale_method = 'maxabs', plot_split = 0.3, data_split=0.2, embedding_size = 256, dropout = False, city=\"\"):\n",
    "    if scale_method == 'default':\n",
    "        print(\"Default scaling method not supported for LSTM... switching to maxabs\")\n",
    "        scale_method = 'maxabs'\n",
    "    X, y = clean_data(data)\n",
    "    # y_max = y.max()\n",
    "    X, y, scaler = preprocess_data(X, y, scale_method)\n",
    "    X_train = X[:int(data_split * len(X))]\n",
    "    X_test = X[int(data_split * len(X)):]\n",
    "    y_train = y[:int(data_split * len(y))]\n",
    "    y_test = y[int(data_split * len(y)):]\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(embedding_size, input_shape=(X_train.shape[1], X_train.shape[2]),))\n",
    "    if dropout:\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[es], verbose=0)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    print_model_performance(mse, r2, mae, mape, model_name='LSTM', scale_method=scale_method)\n",
    "    plot_predictions(y_test, y_pred, scale_method=scale_method, model_name='LSTM', split=plot_split, city=city)\n",
    "    log_results('LSTM', scale_method, data_split, mse, r2, mae, mape, city=city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "def arima_model(data, scale_method = 'minmax', plot_split = 0.3, data_split=0.2, order=(5,1,0), city=\"\"):\n",
    "    if scale_method == 'default':\n",
    "        raise ValueError(\"SARIMAX model does not support 'default' scaling method\")\n",
    "    X, y = clean_data(data)\n",
    "    # y_max = y.max()\n",
    "    X, y, scaler = preprocess_data(X, y, scale_method)\n",
    "    X_train = X[:int(data_split * len(X))]\n",
    "    X_test = X[int(data_split * len(X)):]\n",
    "    y_train = y[:int(data_split * len(y))]\n",
    "    y_test = y[int(data_split * len(y)):]\n",
    "    model = SARIMAX(y_train, order=order)\n",
    "    model_fit = model.fit(disp=0)\n",
    "    y_pred = model_fit.predict(start=len(y_train), end=len(y_train)+len(y_test)-1, dynamic=False)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    print_model_performance(mse, r2, mae, mape, model_name='SARIMAX', scale_method=scale_method)\n",
    "    plot_predictions(y_test, y_pred, scale_method=scale_method, model_name='SARIMAX', split=plot_split, city=city)\n",
    "    log_results('SARIMAX', scale_method, data_split, mse, r2, mae, mape, city=city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "def holt_winters_model(data, scale_method = 'default', plot_split = 0.3, data_split=0.2, city=\"\"):\n",
    "    # if scale_method == 'default':\n",
    "    #     raise ValueError(\"Holt-Winters model does not support 'default' scaling method\")\n",
    "    X, y = clean_data(data)\n",
    "    # y_max = y.max()\n",
    "    X, y, scaler = preprocess_data(X, y, scale_method)\n",
    "    X_train = X[:int(data_split * len(X))]\n",
    "    X_test = X[int(data_split * len(X)):]\n",
    "    y_train = y[:int(data_split * len(y))]\n",
    "    y_test = y[int(data_split * len(y)):]\n",
    "    model = ExponentialSmoothing(y_train, trend='add', seasonal='add', seasonal_periods=366)\n",
    "    model_fit = model.fit()\n",
    "    y_pred = model_fit.predict(start=len(y_train), end=len(y_train)+len(y_test)-1)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    print_model_performance(mse, r2, mae, mape, model_name='Holt-Winters', scale_method=scale_method)\n",
    "    plot_predictions(y_test, y_pred, model_name='Holt-Winters', scale_method=scale_method, split=plot_split, city=city)\n",
    "    log_results('Holt-Winters', scale_method, data_split, mse, r2, mae, mape, city=city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemment RNN model \n",
    "from keras.layers import SimpleRNN\n",
    "\n",
    "def rnn_model(data, scale_method = 'default', plot_split = 0.3, data_split=0.2, city=\"\"):\n",
    "    if scale_method == 'default':\n",
    "        print(\"RNN model does not support 'default' scaling method. Using 'minmax' scaling method\") \n",
    "        scale_method = 'minmax'\n",
    "    X, y = clean_data(data)\n",
    "    # y_max = y.max()\n",
    "    X, y, scaler = preprocess_data(X, y, scale_method)\n",
    "    X_train = X[:int(data_split * len(X))]\n",
    "    X_test = X[int(data_split * len(X)):]\n",
    "    y_train = y[:int(data_split * len(y))]\n",
    "    y_test = y[int(data_split * len(y)):]\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(312, input_shape=(X_train.shape[1], X_train.shape[2]),))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[es], verbose=0)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    print_model_performance(mse, r2, mae, mape, model_name='RNN', scale_method=scale_method)\n",
    "    plot_predictions(y_test, y_pred, scale_method=scale_method, model_name='RNN', split=plot_split, city=city)\n",
    "    log_results('RNN', scale_method, data_split, mse, r2, mae, mape, city=city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement LTC regressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ncps.wirings import AutoNCP\n",
    "from ncps.torch import LTC\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SequenceLearner(pl.LightningModule):\n",
    "    def __init__(self, model, lr=0.005):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat, _ = self.model.forward(x)\n",
    "        y_hat = y_hat.view_as(y)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat, _ = self.model.forward(x)\n",
    "        y_hat = y_hat.view_as(y)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "def ltc_model(data, scale_method = 'maxabs', plot_split = 0.3, data_split=0.2, embedding_size = 256, dropout = False, city=\"\"):\n",
    "    # if scale_method == 'default':\n",
    "    #     print(\"Default scaling method not supported for LTC... switching to maxabs\")\n",
    "    #     scale_method = 'maxabs'\n",
    "    X, y = clean_data(data)\n",
    "    X, y, scaler = preprocess_data(X, y, scale_method)\n",
    "    X_train = X[:int(data_split * len(X))]\n",
    "    X_test = X[int(data_split * len(X)):]\n",
    "    y_train = y[:int(data_split * len(y))]\n",
    "    y_test = y[int(data_split * len(y)):]\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    X_test = torch.Tensor(X_test)\n",
    "    y_train = torch.Tensor(y_train)\n",
    "    y_test = torch.Tensor(y_test)\n",
    "    dataloader = DataLoader(\n",
    "        TensorDataset(X_train, y_train), batch_size=1, shuffle=True, num_workers=4\n",
    "    )\n",
    "    out_features = 1\n",
    "    in_features = X_train.shape[1]\n",
    "    wiring = AutoNCP(embedding_size, out_features)\n",
    "    ltc_model = LTC(in_features, wiring, batch_first=True)\n",
    "    learn = SequenceLearner(ltc_model, lr=0.01)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        gradient_clip_val=1,\n",
    "    )\n",
    "    trainer.fit(learn, dataloader)\n",
    "    y_pred, _ = ltc_model.forward(X_test)\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    # mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    mape=0\n",
    "    print_model_performance(mse, r2, mae, mape, model_name='LTC', scale_method=scale_method)\n",
    "    plot_predictions(y_test, y_pred, scale_method=scale_method, model_name='LTC', split=plot_split, city=city)\n",
    "    log_results('LTC', scale_method, data_split, mse, r2, mae, mape, city=city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, *dims, contiguous=False): \n",
    "        super().__init__()\n",
    "        self.dims, self.contiguous = dims, contiguous\n",
    "    def forward(self, x):\n",
    "        if self.contiguous: return x.transpose(*self.dims).contiguous()\n",
    "        else: return x.transpose(*self.dims)\n",
    "\n",
    "    \n",
    "def get_activation_fn(activation):\n",
    "    if callable(activation): return activation()\n",
    "    elif activation.lower() == \"relu\": return nn.ReLU()\n",
    "    elif activation.lower() == \"gelu\": return nn.GELU()\n",
    "    raise ValueError(f'{activation} is not available. You can use \"relu\", \"gelu\", or a callable') \n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "def PositionalEncoding(q_len, d_model, normalize=True):\n",
    "    pe = torch.zeros(q_len, d_model)\n",
    "    position = torch.arange(0, q_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    if normalize:\n",
    "        pe = pe - pe.mean()\n",
    "        pe = pe / (pe.std() * 10)\n",
    "    return pe\n",
    "\n",
    "SinCosPosEncoding = PositionalEncoding\n",
    "\n",
    "def Coord2dPosEncoding(q_len, d_model, exponential=False, normalize=True, eps=1e-3, verbose=False):\n",
    "    x = .5 if exponential else 1\n",
    "    i = 0\n",
    "    for i in range(100):\n",
    "        cpe = 2 * (torch.linspace(0, 1, q_len).reshape(-1, 1) ** x) * (torch.linspace(0, 1, d_model).reshape(1, -1) ** x) - 1\n",
    "        pv(f'{i:4.0f}  {x:5.3f}  {cpe.mean():+6.3f}', verbose)\n",
    "        if abs(cpe.mean()) <= eps: break\n",
    "        elif cpe.mean() > eps: x += .001\n",
    "        else: x -= .001\n",
    "        i += 1\n",
    "    if normalize:\n",
    "        cpe = cpe - cpe.mean()\n",
    "        cpe = cpe / (cpe.std() * 10)\n",
    "    return cpe\n",
    "\n",
    "def Coord1dPosEncoding(q_len, exponential=False, normalize=True):\n",
    "    cpe = (2 * (torch.linspace(0, 1, q_len).reshape(-1, 1)**(.5 if exponential else 1)) - 1)\n",
    "    if normalize:\n",
    "        cpe = cpe - cpe.mean()\n",
    "        cpe = cpe / (cpe.std() * 10)\n",
    "    return cpe\n",
    "\n",
    "def positional_encoding(pe, learn_pe, q_len, d_model):\n",
    "    # Positional encoding\n",
    "    if pe == None:\n",
    "        W_pos = torch.empty((q_len, d_model)) # pe = None and learn_pe = False can be used to measure impact of pe\n",
    "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
    "        learn_pe = False\n",
    "    elif pe == 'zero':\n",
    "        W_pos = torch.empty((q_len, 1))\n",
    "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
    "    elif pe == 'zeros':\n",
    "        W_pos = torch.empty((q_len, d_model))\n",
    "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
    "    elif pe == 'normal' or pe == 'gauss':\n",
    "        W_pos = torch.zeros((q_len, 1))\n",
    "        torch.nn.init.normal_(W_pos, mean=0.0, std=0.1)\n",
    "    elif pe == 'uniform':\n",
    "        W_pos = torch.zeros((q_len, 1))\n",
    "        nn.init.uniform_(W_pos, a=0.0, b=0.1)\n",
    "    elif pe == 'lin1d': W_pos = Coord1dPosEncoding(q_len, exponential=False, normalize=True)\n",
    "    elif pe == 'exp1d': W_pos = Coord1dPosEncoding(q_len, exponential=True, normalize=True)\n",
    "    elif pe == 'lin2d': W_pos = Coord2dPosEncoding(q_len, d_model, exponential=False, normalize=True)\n",
    "    elif pe == 'exp2d': W_pos = Coord2dPosEncoding(q_len, d_model, exponential=True, normalize=True)\n",
    "    elif pe == 'sincos': W_pos = PositionalEncoding(q_len, d_model, normalize=True)\n",
    "    else: raise ValueError(f\"{pe} is not a valid pe (positional encoder. Available types: 'gauss'=='normal', \\\n",
    "        'zeros', 'zero', uniform', 'lin1d', 'exp1d', 'lin2d', 'exp2d', 'sincos', None.)\")\n",
    "    return nn.Parameter(W_pos, requires_grad=learn_pe)\n",
    "\n",
    "from typing import Callable, Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n",
    "        \"\"\"\n",
    "        :param num_features: the number of features or channels\n",
    "        :param eps: a value added for numerical stability\n",
    "        :param affine: if True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode:str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else: raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim-1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:,-1,:].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        if self.subtract_last:\n",
    "            x = x - self.last\n",
    "        else:\n",
    "            x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps*self.eps)\n",
    "        x = x * self.stdev\n",
    "        if self.subtract_last:\n",
    "            x = x + self.last\n",
    "        else:\n",
    "            x = x + self.mean\n",
    "        return x\n",
    "\n",
    "# Cell\n",
    "class PatchTST_backbone(nn.Module):\n",
    "    def __init__(self, c_in:int, context_window:int, target_window:int, patch_len:int, stride:int, max_seq_len:Optional[int]=1024, \n",
    "                 n_layers:int=3, d_model=128, n_heads=16, d_k:Optional[int]=None, d_v:Optional[int]=None,\n",
    "                 d_ff:int=256, norm:str='BatchNorm', attn_dropout:float=0., dropout:float=0., act:str=\"gelu\", key_padding_mask:bool='auto',\n",
    "                 padding_var:Optional[int]=None, attn_mask:Optional[Tensor]=None, res_attention:bool=True, pre_norm:bool=False, store_attn:bool=False,\n",
    "                 pe:str='zeros', learn_pe:bool=True, fc_dropout:float=0., head_dropout = 0, padding_patch = None,\n",
    "                 pretrain_head:bool=False, head_type = 'flatten', individual = False, revin = True, affine = True, subtract_last = False,\n",
    "                 verbose:bool=False, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # RevIn\n",
    "        self.revin = revin\n",
    "        if self.revin: self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last)\n",
    "        \n",
    "        # Patching\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.padding_patch = padding_patch\n",
    "        patch_num = int((context_window - patch_len)/stride + 1)\n",
    "        if padding_patch == 'end': # can be modified to general case\n",
    "            self.padding_patch_layer = nn.ReplicationPad1d((0, stride)) \n",
    "            patch_num += 1\n",
    "        \n",
    "        # Backbone \n",
    "        self.backbone = TSTiEncoder(c_in, patch_num=patch_num, patch_len=patch_len, max_seq_len=max_seq_len,\n",
    "                                n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n",
    "                                attn_dropout=attn_dropout, dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var,\n",
    "                                attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
    "                                pe=pe, learn_pe=learn_pe, verbose=verbose, **kwargs)\n",
    "\n",
    "        # Head\n",
    "        self.head_nf = d_model * patch_num\n",
    "        self.n_vars = c_in\n",
    "        self.pretrain_head = pretrain_head\n",
    "        self.head_type = head_type\n",
    "        self.individual = individual\n",
    "\n",
    "        if self.pretrain_head: \n",
    "            self.head = self.create_pretrain_head(self.head_nf, c_in, fc_dropout) # custom head passed as a partial func with all its kwargs\n",
    "        elif head_type == 'flatten': \n",
    "            self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, target_window, head_dropout=head_dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, z):                                                                   # z: [bs x nvars x seq_len]\n",
    "        # norm\n",
    "        if self.revin: \n",
    "            z = z.permute(0,2,1)\n",
    "            z = self.revin_layer(z, 'norm')\n",
    "            z = z.permute(0,2,1)\n",
    "            \n",
    "        # do patching\n",
    "        if self.padding_patch == 'end':\n",
    "            z = self.padding_patch_layer(z)\n",
    "        z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)                   # z: [bs x nvars x patch_num x patch_len]\n",
    "        z = z.permute(0,1,3,2)                                                              # z: [bs x nvars x patch_len x patch_num]\n",
    "        \n",
    "        # model\n",
    "        z = self.backbone(z)                                                                # z: [bs x nvars x d_model x patch_num]\n",
    "        z = self.head(z)                                                                    # z: [bs x nvars x target_window] \n",
    "        \n",
    "        # denorm\n",
    "        if self.revin: \n",
    "            z = z.permute(0,2,1)\n",
    "            z = self.revin_layer(z, 'denorm')\n",
    "            z = z.permute(0,2,1)\n",
    "        return z\n",
    "    \n",
    "    def create_pretrain_head(self, head_nf, vars, dropout):\n",
    "        return nn.Sequential(nn.Dropout(dropout),\n",
    "                    nn.Conv1d(head_nf, vars, 1)\n",
    "                    )\n",
    "\n",
    "class Flatten_Head(nn.Module):\n",
    "    def __init__(self, individual, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.individual = individual\n",
    "        self.n_vars = n_vars\n",
    "        \n",
    "        if self.individual:\n",
    "            self.linears = nn.ModuleList()\n",
    "            self.dropouts = nn.ModuleList()\n",
    "            self.flattens = nn.ModuleList()\n",
    "            for i in range(self.n_vars):\n",
    "                self.flattens.append(nn.Flatten(start_dim=-2))\n",
    "                self.linears.append(nn.Linear(nf, target_window))\n",
    "                self.dropouts.append(nn.Dropout(head_dropout))\n",
    "        else:\n",
    "            self.flatten = nn.Flatten(start_dim=-2)\n",
    "            self.linear = nn.Linear(nf, target_window)\n",
    "            self.dropout = nn.Dropout(head_dropout)\n",
    "            \n",
    "    def forward(self, x):                                 # x: [bs x nvars x d_model x patch_num]\n",
    "        if self.individual:\n",
    "            x_out = []\n",
    "            for i in range(self.n_vars):\n",
    "                z = self.flattens[i](x[:,i,:,:])          # z: [bs x d_model * patch_num]\n",
    "                z = self.linears[i](z)                    # z: [bs x target_window]\n",
    "                z = self.dropouts[i](z)\n",
    "                x_out.append(z)\n",
    "            x = torch.stack(x_out, dim=1)                 # x: [bs x nvars x target_window]\n",
    "        else:\n",
    "            x = self.flatten(x)\n",
    "            x = self.linear(x)\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "class TSTiEncoder(nn.Module):  #i means channel-independent\n",
    "    def __init__(self, c_in, patch_num, patch_len, max_seq_len=1024,\n",
    "                 n_layers=3, d_model=128, n_heads=16, d_k=None, d_v=None,\n",
    "                 d_ff=256, norm='BatchNorm', attn_dropout=0., dropout=0., act=\"gelu\", store_attn=False,\n",
    "                 key_padding_mask='auto', padding_var=None, attn_mask=None, res_attention=True, pre_norm=False,\n",
    "                 pe='zeros', learn_pe=True, verbose=False, **kwargs):\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_num = patch_num\n",
    "        self.patch_len = patch_len\n",
    "        \n",
    "        # Input encoding\n",
    "        q_len = patch_num\n",
    "        self.W_P = nn.Linear(patch_len, d_model)        # Eq 1: projection of feature vectors onto a d-dim vector space\n",
    "        self.seq_len = q_len\n",
    "\n",
    "        # Positional encoding\n",
    "        self.W_pos = positional_encoding(pe, learn_pe, q_len, d_model)\n",
    "\n",
    "        # Residual dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = TSTEncoder(q_len, d_model, n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout, dropout=dropout,\n",
    "                                   pre_norm=pre_norm, activation=act, res_attention=res_attention, n_layers=n_layers, store_attn=store_attn)\n",
    "\n",
    "        \n",
    "    def forward(self, x) -> Tensor:                                              # x: [bs x nvars x patch_len x patch_num]\n",
    "        \n",
    "        n_vars = x.shape[1]\n",
    "        # Input encoding\n",
    "        x = x.permute(0,1,3,2)                                                   # x: [bs x nvars x patch_num x patch_len]\n",
    "        x = self.W_P(x)                                                          # x: [bs x nvars x patch_num x d_model]\n",
    "\n",
    "        u = torch.reshape(x, (x.shape[0]*x.shape[1],x.shape[2],x.shape[3]))      # u: [bs * nvars x patch_num x d_model]\n",
    "        u = self.dropout(u + self.W_pos)                                         # u: [bs * nvars x patch_num x d_model]\n",
    "\n",
    "        # Encoder\n",
    "        z = self.encoder(u)                                                      # z: [bs * nvars x patch_num x d_model]\n",
    "        z = torch.reshape(z, (-1,n_vars,z.shape[-2],z.shape[-1]))                # z: [bs x nvars x patch_num x d_model]\n",
    "        z = z.permute(0,1,3,2)                                                   # z: [bs x nvars x d_model x patch_num]\n",
    "        \n",
    "        return z    \n",
    "            \n",
    "            \n",
    "    \n",
    "# Cell\n",
    "class TSTEncoder(nn.Module):\n",
    "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=None, \n",
    "                        norm='BatchNorm', attn_dropout=0., dropout=0., activation='gelu',\n",
    "                        res_attention=False, n_layers=1, pre_norm=False, store_attn=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([TSTEncoderLayer(q_len, d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm,\n",
    "                                                      attn_dropout=attn_dropout, dropout=dropout,\n",
    "                                                      activation=activation, res_attention=res_attention,\n",
    "                                                      pre_norm=pre_norm, store_attn=store_attn) for i in range(n_layers)])\n",
    "        self.res_attention = res_attention\n",
    "\n",
    "    def forward(self, src:Tensor, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
    "        output = src\n",
    "        scores = None\n",
    "        if self.res_attention:\n",
    "            for mod in self.layers: output, scores = mod(output, prev=scores, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "            return output\n",
    "        else:\n",
    "            for mod in self.layers: output = mod(output, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "            return output\n",
    "\n",
    "\n",
    "\n",
    "class TSTEncoderLayer(nn.Module):\n",
    "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=256, store_attn=False,\n",
    "                 norm='BatchNorm', attn_dropout=0, dropout=0., bias=True, activation=\"gelu\", res_attention=False, pre_norm=False):\n",
    "        super().__init__()\n",
    "        assert not d_model%n_heads, f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
    "        d_k = d_model // n_heads if d_k is None else d_k\n",
    "        d_v = d_model // n_heads if d_v is None else d_v\n",
    "\n",
    "        # Multi-Head attention\n",
    "        self.res_attention = res_attention\n",
    "        self.self_attn = _MultiheadAttention(d_model, n_heads, d_k, d_v, attn_dropout=attn_dropout, proj_dropout=dropout, res_attention=res_attention)\n",
    "\n",
    "        # Add & Norm\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "        if \"batch\" in norm.lower():\n",
    "            self.norm_attn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
    "        else:\n",
    "            self.norm_attn = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Position-wise Feed-Forward\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model, d_ff, bias=bias),\n",
    "                                get_activation_fn(activation),\n",
    "                                nn.Dropout(dropout),\n",
    "                                nn.Linear(d_ff, d_model, bias=bias))\n",
    "\n",
    "        # Add & Norm\n",
    "        self.dropout_ffn = nn.Dropout(dropout)\n",
    "        if \"batch\" in norm.lower():\n",
    "            self.norm_ffn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
    "        else:\n",
    "            self.norm_ffn = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "        self.store_attn = store_attn\n",
    "\n",
    "\n",
    "    def forward(self, src:Tensor, prev:Optional[Tensor]=None, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None) -> Tensor:\n",
    "\n",
    "        # Multi-Head attention sublayer\n",
    "        if self.pre_norm:\n",
    "            src = self.norm_attn(src)\n",
    "        ## Multi-Head attention\n",
    "        if self.res_attention:\n",
    "            src2, attn, scores = self.self_attn(src, src, src, prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        else:\n",
    "            src2, attn = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        if self.store_attn:\n",
    "            self.attn = attn\n",
    "        ## Add & Norm\n",
    "        src = src + self.dropout_attn(src2) # Add: residual connection with residual dropout\n",
    "        if not self.pre_norm:\n",
    "            src = self.norm_attn(src)\n",
    "\n",
    "        # Feed-forward sublayer\n",
    "        if self.pre_norm:\n",
    "            src = self.norm_ffn(src)\n",
    "        ## Position-wise Feed-Forward\n",
    "        src2 = self.ff(src)\n",
    "        ## Add & Norm\n",
    "        src = src + self.dropout_ffn(src2) # Add: residual connection with residual dropout\n",
    "        if not self.pre_norm:\n",
    "            src = self.norm_ffn(src)\n",
    "\n",
    "        if self.res_attention:\n",
    "            return src, scores\n",
    "        else:\n",
    "            return src\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class _MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_k=None, d_v=None, res_attention=False, attn_dropout=0., proj_dropout=0., qkv_bias=True, lsa=False):\n",
    "        \"\"\"Multi Head Attention Layer\n",
    "        Input shape:\n",
    "            Q:       [batch_size (bs) x max_q_len x d_model]\n",
    "            K, V:    [batch_size (bs) x q_len x d_model]\n",
    "            mask:    [q_len x q_len]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        d_k = d_model // n_heads if d_k is None else d_k\n",
    "        d_v = d_model // n_heads if d_v is None else d_v\n",
    "\n",
    "        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n",
    "\n",
    "        # Scaled Dot-Product Attention (multiple heads)\n",
    "        self.res_attention = res_attention\n",
    "        self.sdp_attn = _ScaledDotProductAttention(d_model, n_heads, attn_dropout=attn_dropout, res_attention=self.res_attention, lsa=lsa)\n",
    "\n",
    "        # Poject output\n",
    "        self.to_out = nn.Sequential(nn.Linear(n_heads * d_v, d_model), nn.Dropout(proj_dropout))\n",
    "\n",
    "\n",
    "    def forward(self, Q:Tensor, K:Optional[Tensor]=None, V:Optional[Tensor]=None, prev:Optional[Tensor]=None,\n",
    "                key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
    "\n",
    "        bs = Q.size(0)\n",
    "        if K is None: K = Q\n",
    "        if V is None: V = Q\n",
    "\n",
    "        # Linear (+ split in multiple heads)\n",
    "        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1,2)       # q_s    : [bs x n_heads x max_q_len x d_k]\n",
    "        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0,2,3,1)     # k_s    : [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n",
    "        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1,2)       # v_s    : [bs x n_heads x q_len x d_v]\n",
    "\n",
    "        # Apply Scaled Dot-Product Attention (multiple heads)\n",
    "        if self.res_attention:\n",
    "            output, attn_weights, attn_scores = self.sdp_attn(q_s, k_s, v_s, prev=prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        else:\n",
    "            output, attn_weights = self.sdp_attn(q_s, k_s, v_s, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n",
    "\n",
    "        # back to the original inputs dimensions\n",
    "        output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v) # output: [bs x q_len x n_heads * d_v]\n",
    "        output = self.to_out(output)\n",
    "\n",
    "        if self.res_attention: return output, attn_weights, attn_scores\n",
    "        else: return output, attn_weights\n",
    "\n",
    "\n",
    "class _ScaledDotProductAttention(nn.Module):\n",
    "    r\"\"\"Scaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with optional residual attention from previous layer\n",
    "    (Realformer: Transformer likes residual attention by He et al, 2020) and locality self sttention (Vision Transformer for Small-Size Datasets\n",
    "    by Lee et al, 2021)\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, n_heads, attn_dropout=0., res_attention=False, lsa=False):\n",
    "        super().__init__()\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.res_attention = res_attention\n",
    "        head_dim = d_model // n_heads\n",
    "        self.scale = nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n",
    "        self.lsa = lsa\n",
    "\n",
    "    def forward(self, q:Tensor, k:Tensor, v:Tensor, prev:Optional[Tensor]=None, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
    "        '''\n",
    "        Input shape:\n",
    "            q               : [bs x n_heads x max_q_len x d_k]\n",
    "            k               : [bs x n_heads x d_k x seq_len]\n",
    "            v               : [bs x n_heads x seq_len x d_v]\n",
    "            prev            : [bs x n_heads x q_len x seq_len]\n",
    "            key_padding_mask: [bs x seq_len]\n",
    "            attn_mask       : [1 x seq_len x seq_len]\n",
    "        Output shape:\n",
    "            output:  [bs x n_heads x q_len x d_v]\n",
    "            attn   : [bs x n_heads x q_len x seq_len]\n",
    "            scores : [bs x n_heads x q_len x seq_len]\n",
    "        '''\n",
    "\n",
    "        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n",
    "        attn_scores = torch.matmul(q, k) * self.scale      # attn_scores : [bs x n_heads x max_q_len x q_len]\n",
    "\n",
    "        # Add pre-softmax attention scores from the previous layer (optional)\n",
    "        if prev is not None: attn_scores = attn_scores + prev\n",
    "\n",
    "        # Attention mask (optional)\n",
    "        if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_scores.masked_fill_(attn_mask, -np.inf)\n",
    "            else:\n",
    "                attn_scores += attn_mask\n",
    "\n",
    "        # Key padding mask (optional)\n",
    "        if key_padding_mask is not None:                              # mask with shape [bs x q_len] (only when max_w_len == q_len)\n",
    "            attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf)\n",
    "\n",
    "        # normalize the attention weights\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)                 # attn_weights   : [bs x n_heads x max_q_len x q_len]\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # compute the new values given the attention weights\n",
    "        output = torch.matmul(attn_weights, v)                        # output: [bs x n_heads x max_q_len x d_v]\n",
    "\n",
    "        if self.res_attention: return output, attn_weights, attn_scores\n",
    "        else: return output, attn_weights\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, configs, max_seq_len:Optional[int]=1024, d_k:Optional[int]=None, d_v:Optional[int]=None, norm:str='BatchNorm', attn_dropout:float=0., \n",
    "                 act:str=\"gelu\", key_padding_mask:bool='auto',padding_var:Optional[int]=None, attn_mask:Optional[Tensor]=None, res_attention:bool=True, \n",
    "                 pre_norm:bool=False, store_attn:bool=False, pe:str='zeros', learn_pe:bool=True, pretrain_head:bool=False, head_type = 'flatten', verbose:bool=False, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # load parameters\n",
    "        c_in = configs.enc_in\n",
    "        context_window = configs.seq_len\n",
    "        target_window = configs.pred_len\n",
    "        \n",
    "        n_layers = configs.e_layers\n",
    "        n_heads = configs.n_heads\n",
    "        d_model = configs.d_model\n",
    "        d_ff = configs.d_ff\n",
    "        dropout = configs.dropout\n",
    "        fc_dropout = configs.fc_dropout\n",
    "        head_dropout = configs.head_dropout\n",
    "        \n",
    "        individual = configs.individual\n",
    "    \n",
    "        patch_len = configs.patch_len\n",
    "        stride = configs.stride\n",
    "        padding_patch = configs.padding_patch\n",
    "        \n",
    "        revin = configs.revin\n",
    "        affine = configs.affine\n",
    "        subtract_last = configs.subtract_last\n",
    "        \n",
    "        decomposition = configs.decomposition\n",
    "        kernel_size = configs.kernel_size\n",
    "        \n",
    "        \n",
    "        # model\n",
    "        self.decomposition = decomposition\n",
    "        if self.decomposition:\n",
    "            self.decomp_module = series_decomp(kernel_size)\n",
    "            self.model_trend = PatchTST_backbone(c_in=c_in, context_window = context_window, target_window=target_window, patch_len=patch_len, stride=stride, \n",
    "                                  max_seq_len=max_seq_len, n_layers=n_layers, d_model=d_model,\n",
    "                                  n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n",
    "                                  dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var, \n",
    "                                  attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
    "                                  pe=pe, learn_pe=learn_pe, fc_dropout=fc_dropout, head_dropout=head_dropout, padding_patch = padding_patch,\n",
    "                                  pretrain_head=pretrain_head, head_type=head_type, individual=individual, revin=revin, affine=affine,\n",
    "                                  subtract_last=subtract_last, verbose=verbose, **kwargs)\n",
    "            self.model_res = PatchTST_backbone(c_in=c_in, context_window = context_window, target_window=target_window, patch_len=patch_len, stride=stride, \n",
    "                                  max_seq_len=max_seq_len, n_layers=n_layers, d_model=d_model,\n",
    "                                  n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n",
    "                                  dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var, \n",
    "                                  attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
    "                                  pe=pe, learn_pe=learn_pe, fc_dropout=fc_dropout, head_dropout=head_dropout, padding_patch = padding_patch,\n",
    "                                  pretrain_head=pretrain_head, head_type=head_type, individual=individual, revin=revin, affine=affine,\n",
    "                                  subtract_last=subtract_last, verbose=verbose, **kwargs)\n",
    "        else:\n",
    "            self.model = PatchTST_backbone(c_in=c_in, context_window = context_window, target_window=target_window, patch_len=patch_len, stride=stride, \n",
    "                                  max_seq_len=max_seq_len, n_layers=n_layers, d_model=d_model,\n",
    "                                  n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n",
    "                                  dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var, \n",
    "                                  attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
    "                                  pe=pe, learn_pe=learn_pe, fc_dropout=fc_dropout, head_dropout=head_dropout, padding_patch = padding_patch,\n",
    "                                  pretrain_head=pretrain_head, head_type=head_type, individual=individual, revin=revin, affine=affine,\n",
    "                                  subtract_last=subtract_last, verbose=verbose, **kwargs)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):           # x: [Batch, Input length, Channel]\n",
    "        if self.decomposition:\n",
    "            res_init, trend_init = self.decomp_module(x)\n",
    "            res_init, trend_init = res_init.permute(0,2,1), trend_init.permute(0,2,1)  # x: [Batch, Channel, Input length]\n",
    "            res = self.model_res(res_init)\n",
    "            trend = self.model_trend(trend_init)\n",
    "            x = res + trend\n",
    "            x = x.permute(0,2,1)    # x: [Batch, Input length, Channel]\n",
    "        else:\n",
    "            x = x.permute(0,2,1)    # x: [Batch, Channel, Input length]\n",
    "            x = self.model(x)\n",
    "            x = x.permute(0,2,1)    # x: [Batch, Input length, Channel]\n",
    "        return x\n",
    "    \n",
    "def patch_tst_model(data, configs, scale_method = 'default', plot_split = 0.3, data_split=0.2, city=\"\"):\n",
    "    if scale_method == 'default':\n",
    "        print(\"PatchTST model does not support 'default' scaling method.\")\n",
    "        scale_method = \"minmax\"\n",
    "    X, y = clean_data(data)\n",
    "    X, y, scaler = preprocess_data(X, y, scale_method)\n",
    "    X_train = X[:int(data_split * len(X))]\n",
    "    X_test = X[int(data_split * len(X)):]\n",
    "    y_train = y[:int(data_split * len(y))]\n",
    "    y_test = y[int(data_split * len(y)):]\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    # X_train = torch.Tensor(X_train, dtype=torch.float32)\n",
    "    # X_test = torch.Tensor(X_test, dtype=torch.float32)\n",
    "    # y_train = torch.Tensor(y_train, dtype=torch.float32)\n",
    "    # y_test = torch.Tensor(y_test, dtype=torch.float32)\n",
    "    model = Model(configs)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(torch.from_numpy(X_train).float())\n",
    "        loss = criterion(outputs, torch.from_numpy(y_train).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    y_pred = model(torch.from_numpy(X_test).float()).detach().numpy()\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    print_model_performance(mse, r2, mae, mape, model_name='PatchTST', scale_method=scale_method)\n",
    "    plot_predictions(y_test, y_pred, scale_method=scale_method, model_name='PatchTST', split=plot_split, city=city)\n",
    "    log_results('PatchTST', scale_method, data_split, mse, r2, mae, mape, city=city)\n",
    "    \n",
    "class Configs:\n",
    "    def __init__(self):\n",
    "        self.enc_in = 10  # number of features\n",
    "        self.seq_len = 100  # length of the sequence\n",
    "        self.pred_len = 1  # prediction horizon\n",
    "        self.e_layers = 3  # number of encoder layers\n",
    "        self.n_heads = 8  # number of attention heads\n",
    "        self.d_model = 512  # dimension of the model\n",
    "        self.d_ff = 2048  # dimension of the feedforward network\n",
    "        self.dropout = 0.1  # dropout rate\n",
    "        self.fc_dropout = 0.1  # dropout rate for the fully connected layer\n",
    "        self.head_dropout = 0.1  # dropout rate for the head\n",
    "        self.individual = False  # whether to use individual normalization\n",
    "        self.patch_len = 1  # length of the patches\n",
    "        self.stride = 1  # stride for the patches\n",
    "        self.padding_patch = 0  # padding for the patches\n",
    "        self.revin = False  # whether to reverse the input sequence\n",
    "        self.affine = True  # whether to use affine transformation in the normalization layers\n",
    "        self.subtract_last = False  # whether to subtract the last value in each patch\n",
    "        self.decomposition = False  # whether to use series decomposition\n",
    "        self.kernel_size = 5  # kernel size for the series decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def run_all_methods_on_model(data, model_name, data_split=0.2, plot_split=0.3, city=\"\"):\n",
    "    scalers = ['default', 'standard', 'minmax', 'maxabs', 'robust']\n",
    "    configs = Configs()\n",
    "    for scaler in scalers:\n",
    "        # if model_name == 'sarimax':\n",
    "        #     if scaler == 'default':\n",
    "        #         print(\"default scaler not supported for SARIMAX model...switching to standard scaler\")\n",
    "        #         scaler = 'standard'\n",
    "        #     arima_model(data, scaler, data_split=data_split, plot_split=plot_split, city=city)\n",
    "        # elif model_name == 'holt_winters':\n",
    "        #     holt_winters_model(data, scaler, data_split=data_split, plot_split=plot_split, city=city)\n",
    "        if model_name == 'linear':\n",
    "            linear_regression_model(data, scaler, data_split=data_split, plot_split=plot_split, city=city)\n",
    "        elif model_name == 'nn':\n",
    "            nn_model(data, scaler, data_split=data_split, plot_split=plot_split, city=city)\n",
    "        elif model_name == 'lstm':\n",
    "            lstm_model(data, scaler, data_split=data_split, plot_split=plot_split, city=city)\n",
    "        elif model_name == 'rnn':\n",
    "            rnn_model(data, scaler, data_split=data_split, plot_split=plot_split, city=city)\n",
    "        elif model_name == 'ltc':\n",
    "            ltc_model(data, scaler, data_split=data_split, plot_split=plot_split, city=city)\n",
    "        elif model_name == 'patchtst':\n",
    "            patch_tst_model(data, configs, scaler, data_split=data_split, plot_split=plot_split, city=city)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model type\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_day = pd.read_csv('city_day.csv')\n",
    "amaravati, amritsar, chandigarh, delhi, gurugram, hyderabad, kolkata, patna, visakhapatnam = get_all_cities(city_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = ['linear', 'nn', 'lstm', 'rnn',]\n",
    "# for model in models:\n",
    "#     run_all_methods_on_model(delhi, model, data_split=0.2, plot_split=0.3, city=\"Delhi\")\n",
    "    # run_all_methods_on_model(gurugram, model, data_split=0.2, plot_split=0.3, city=\"Gurugram\")\n",
    "    # run_all_methods_on_model(hyderabad, model, data_split=0.2, plot_split=0.3, city=\"Hyderabad\")\n",
    "    # run_all_methods_on_model(kolkata, model, data_split=0.2, plot_split=0.3, city=\"Kolkata\")\n",
    "    # run_all_methods_on_model(patna, model, data_split=0.2, plot_split=0.3, city=\"Patna\")\n",
    "    # run_all_methods_on_model(visakhapatnam, model, data_split=0.2, plot_split=0.3, city=\"Visakhapatnam\")\n",
    "    # run_all_methods_on_model(amaravati, model, data_split=0.2, plot_split=0.3, city=\"Amaravati\")\n",
    "    # run_all_methods_on_model(amritsar, model, data_split=0.2, plot_split=0.3, city=\"Amritsar\")\n",
    "    # run_all_methods_on_model(chandigarh, model, data_split=0.2, plot_split=0.3, city=\"Chandigarh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchTST model does not support 'default' scaling method.\n"
     ]
    }
   ],
   "source": [
    "run_all_methods_on_model(delhi, 'patchtst', data_split=0.2, plot_split=0.3, city=\"Delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
